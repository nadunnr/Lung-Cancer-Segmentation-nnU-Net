{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bbcc0c-6830-4fc2-868c-0312bf88e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8a0917-63d2-45ab-b865-253a72e1624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is DATA\n",
      " Volume Serial Number is 26BD-BE8F\n",
      "\n",
      " Directory of D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\n",
      "\n",
      "03/10/2024  02:08 PM    <DIR>          .\n",
      "02/01/2024  11:02 PM    <DIR>          ..\n",
      "02/01/2024  11:03 PM               325 dataset.json\n",
      "02/01/2024  11:03 PM            11,120 dataset_fingerprint.json\n",
      "02/01/2024  11:43 PM    <DIR>          gt_segmentations\n",
      "02/01/2024  11:03 PM            11,892 nnUNetPlans.json\n",
      "03/24/2024  09:27 AM    <DIR>          nnUNetPlans_2d\n",
      "02/01/2024  11:37 PM    <DIR>          nnUNetPlans_3d_fullres\n",
      "03/10/2024  02:08 PM    <DIR>          nnUNetPlans_3d_lowres\n",
      "03/10/2024  02:08 PM             8,123 splits_final.json\n",
      "               4 File(s)         31,460 bytes\n",
      "               6 Dir(s)  55,029,248,000 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45445415-ade1-4e07-a262-c01404d94cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nnUNetv2_train [-h] [-tr TR] [-p P]\n",
      "                      [-pretrained_weights PRETRAINED_WEIGHTS]\n",
      "                      [-num_gpus NUM_GPUS] [--use_compressed] [--npz] [--c]\n",
      "                      [--val] [--val_best] [--disable_checkpointing]\n",
      "                      [-device DEVICE]\n",
      "                      dataset_name_or_id configuration fold\n",
      "\n",
      "positional arguments:\n",
      "  dataset_name_or_id    Dataset name or ID to train with\n",
      "  configuration         Configuration that should be trained\n",
      "  fold                  Fold of the 5-fold cross-validation. Should be an int\n",
      "                        between 0 and 4.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -tr TR                [OPTIONAL] Use this flag to specify a custom trainer.\n",
      "                        Default: nnUNetTrainer\n",
      "  -p P                  [OPTIONAL] Use this flag to specify a custom plans\n",
      "                        identifier. Default: nnUNetPlans\n",
      "  -pretrained_weights PRETRAINED_WEIGHTS\n",
      "                        [OPTIONAL] path to nnU-Net checkpoint file to be used\n",
      "                        as pretrained model. Will only be used when actually\n",
      "                        training. Beta. Use with caution.\n",
      "  -num_gpus NUM_GPUS    Specify the number of GPUs to use for training\n",
      "  --use_compressed      [OPTIONAL] If you set this flag the training cases\n",
      "                        will not be decompressed. Reading compressed data is\n",
      "                        much more CPU and (potentially) RAM intensive and\n",
      "                        should only be used if you know what you are doing\n",
      "  --npz                 [OPTIONAL] Save softmax predictions from final\n",
      "                        validation as npz files (in addition to predicted\n",
      "                        segmentations). Needed for finding the best ensemble.\n",
      "  --c                   [OPTIONAL] Continue training from latest checkpoint\n",
      "  --val                 [OPTIONAL] Set this flag to only run the validation.\n",
      "                        Requires training to have finished.\n",
      "  --val_best            [OPTIONAL] If set, the validation will be performed\n",
      "                        with the checkpoint_best instead of checkpoint_final.\n",
      "                        NOT COMPATIBLE with --disable_checkpointing! WARNING:\n",
      "                        This will use the same 'validation' folder as the\n",
      "                        regular validation with no way of distinguishing the\n",
      "                        two!\n",
      "  --disable_checkpointing\n",
      "                        [OPTIONAL] Set this flag to disable checkpointing.\n",
      "                        Ideal for testing things out and you dont want to\n",
      "                        flood your hard drive with checkpoints.\n",
      "  -device DEVICE        Use this to set the device the training should run\n",
      "                        with. Available options are 'cuda' (GPU), 'cpu' (CPU)\n",
      "                        and 'mps' (Apple M1/M2). Do NOT use this to set which\n",
      "                        GPU ID! Use CUDA_VISIBLE_DEVICES=X nnUNetv2_train\n",
      "                        [...] instead!\n"
     ]
    }
   ],
   "source": [
    "command1 = \"nnUNetv2_train -h\"\n",
    "!{command1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e333978-7751-4a1b-b098-01d9a8d324e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA device(s) available.\n",
      "CUDA device 0: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(f\"Found {num_cuda_devices} CUDA device(s) available.\")\n",
    "    \n",
    "    # Iterate over each CUDA device and print its properties\n",
    "    for i in range(num_cuda_devices):\n",
    "        print(f\"CUDA device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Make sure CUDA drivers are properly installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33ae4c-d61d-4869-b9e5-b38391f32164",
   "metadata": {},
   "source": [
    "Training nnUNet 3d-lowres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b687832-06db-42b3-b680-0d483d075f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7880855202674866, 0.7880855202674866], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [7, 7], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_LungCancer', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.25, 0.7880855202674866, 0.7880855202674866], 'original_median_shape_after_transp': [266, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2378.0, 'mean': -228.04481506347656, 'median': -91.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 297.0, 'std': 327.0248718261719}}} \n",
      "\n",
      "2024-03-24 09:33:00.676940: unpacking dataset...\n",
      "2024-03-24 09:33:01.100950: unpacking done...\n",
      "2024-03-24 09:33:01.105512: do_dummy_2d_data_aug: False\n",
      "2024-03-24 09:33:01.105512: Using splits from existing split file: D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\\splits_final.json\n",
      "2024-03-24 09:33:01.116668: The split file contains 5 splits.\n",
      "2024-03-24 09:33:01.116668: Desired fold for training: 0\n",
      "2024-03-24 09:33:01.125672: This split has 49 training and 13 validation cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\nnUNetv2_train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 268, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 204, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 1234, in run_training\n",
      "    self.on_train_start()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 815, in on_train_start\n",
      "    self.dataloader_train, self.dataloader_val = self.get_dataloaders()\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 614, in get_dataloaders\n",
      "    dl_tr, dl_val = self.get_plain_dataloaders(initial_patch_size, dim)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 640, in get_plain_dataloaders\n",
      "    dl_val = nnUNetDataLoader2D(dataset_val, self.batch_size,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\base_data_loader.py\", line 38, in __init__\n",
      "    self.data_shape, self.seg_shape = self.determine_shapes()\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\base_data_loader.py\", line 57, in determine_shapes\n",
      "    data, seg, properties = self._data.load_case(self.indices[0])\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 81, in load_case\n",
      "    entry = self[key]\n",
      "            ~~~~^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 62, in __getitem__\n",
      "    ret['properties'] = load_pickle(ret['properties_file'])\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\utilities\\file_and_folder_operations.py\", line 57, in load_pickle\n",
      "    with open(file, mode) as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\ScholarX\\\\nnU-Net\\\\nnUNet_preprocessed\\\\Dataset001_LungCancer\\\\nnUNetPlans_2d\\\\lung_006.pkl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in background worker 0:\n",
      " [Errno 2] No such file or directory: 'D:\\\\ScholarX\\\\nnU-Net\\\\nnUNet_preprocessed\\\\Dataset001_LungCancer\\\\nnUNetPlans_2d\\\\lung_015.pkl'\n",
      "Exception in background worker 1:\n",
      " [WinError 8] Not enough memory resources are available to process this command\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7880855202674866, 0.7880855202674866], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [7, 7], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_LungCancer', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.25, 0.7880855202674866, 0.7880855202674866], 'original_median_shape_after_transp': [266, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2378.0, 'mean': -228.04481506347656, 'median': -91.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 297.0, 'std': 327.0248718261719}}} \n",
      "\n",
      "2024-03-24 09:33:07.833275: unpacking dataset...\n",
      "2024-03-24 09:33:08.221419: unpacking done...\n",
      "2024-03-24 09:33:08.228946: do_dummy_2d_data_aug: False\n",
      "2024-03-24 09:33:08.235446: Using splits from existing split file: D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\\splits_final.json\n",
      "2024-03-24 09:33:08.243634: The split file contains 5 splits.\n",
      "2024-03-24 09:33:08.250674: Desired fold for training: 1\n",
      "2024-03-24 09:33:08.259163: This split has 49 training and 13 validation cases.\n",
      "2024-03-24 09:33:13.098489: Unable to plot network architecture:\n",
      "2024-03-24 09:33:13.103512: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n",
      "2024-03-24 09:33:13.146320: \n",
      "2024-03-24 09:33:13.152262: Epoch 0\n",
      "2024-03-24 09:33:13.156255: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 53, in producer\n",
      "    item = next(data_loader)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\data_loader.py\", line 126, in __next__\n",
      "    return self.generate_train_batch()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\data_loader_2d.py\", line 18, in generate_train_batch\n",
      "    data, seg, properties = self._data.load_case(current_key)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 81, in load_case\n",
      "    entry = self[key]\n",
      "            ~~~~^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 62, in __getitem__\n",
      "    ret['properties'] = load_pickle(ret['properties_file'])\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\utilities\\file_and_folder_operations.py\", line 57, in load_pickle\n",
      "    with open(file, mode) as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\ScholarX\\\\nnU-Net\\\\nnUNet_preprocessed\\\\Dataset001_LungCancer\\\\nnUNetPlans_2d\\\\lung_015.pkl'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 53, in producer\n",
      "    item = next(data_loader)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\data_loader.py\", line 126, in __next__\n",
      "    return self.generate_train_batch()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\data_loader_2d.py\", line 18, in generate_train_batch\n",
      "    data, seg, properties = self._data.load_case(current_key)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 86, in load_case\n",
      "    data = np.load(entry['data_file'][:-4] + \".npy\", 'r')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py\", line 453, in load\n",
      "    return format.open_memmap(file, mode=mmap_mode,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\format.py\", line 945, in open_memmap\n",
      "    marray = numpy.memmap(filename, dtype=dtype, shape=shape, order=order,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\memmap.py\", line 268, in __new__\n",
      "    mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [WinError 8] Not enough memory resources are available to process this command\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\nnUNetv2_train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 268, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 204, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 1242, in run_training\n",
      "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 196, in __next__\n",
      "    item = self.__get_next_item()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 181, in __get_next_item\n",
      "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
      "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in background worker 0:\n",
      " [Errno 2] No such file or directory: 'D:\\\\ScholarX\\\\nnU-Net\\\\nnUNet_preprocessed\\\\Dataset001_LungCancer\\\\nnUNetPlans_2d\\\\lung_017.pkl'\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7880855202674866, 0.7880855202674866], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [7, 7], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_LungCancer', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.25, 0.7880855202674866, 0.7880855202674866], 'original_median_shape_after_transp': [266, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2378.0, 'mean': -228.04481506347656, 'median': -91.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 297.0, 'std': 327.0248718261719}}} \n",
      "\n",
      "2024-03-24 09:33:25.460034: unpacking dataset...\n",
      "2024-03-24 09:33:25.844833: unpacking done...\n",
      "2024-03-24 09:33:25.851247: do_dummy_2d_data_aug: False\n",
      "2024-03-24 09:33:25.855813: Using splits from existing split file: D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\\splits_final.json\n",
      "2024-03-24 09:33:25.857211: The split file contains 5 splits.\n",
      "2024-03-24 09:33:25.857211: Desired fold for training: 2\n",
      "2024-03-24 09:33:25.865829: This split has 50 training and 12 validation cases.\n",
      "2024-03-24 09:33:30.341789: Unable to plot network architecture:\n",
      "2024-03-24 09:33:30.342874: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n",
      "2024-03-24 09:33:30.371104: \n",
      "2024-03-24 09:33:30.380258: Epoch 0\n",
      "2024-03-24 09:33:30.386794: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 53, in producer\n",
      "    item = next(data_loader)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\data_loader.py\", line 126, in __next__\n",
      "    return self.generate_train_batch()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\data_loader_2d.py\", line 18, in generate_train_batch\n",
      "    data, seg, properties = self._data.load_case(current_key)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 81, in load_case\n",
      "    entry = self[key]\n",
      "            ~~~~^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 62, in __getitem__\n",
      "    ret['properties'] = load_pickle(ret['properties_file'])\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\utilities\\file_and_folder_operations.py\", line 57, in load_pickle\n",
      "    with open(file, mode) as f:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\ScholarX\\\\nnU-Net\\\\nnUNet_preprocessed\\\\Dataset001_LungCancer\\\\nnUNetPlans_2d\\\\lung_017.pkl'\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\nnUNetv2_train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 268, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 204, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 1242, in run_training\n",
      "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 196, in __next__\n",
      "    item = self.__get_next_item()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 181, in __get_next_item\n",
      "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
      "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in background worker 0:\n",
      " [WinError 8] Not enough memory resources are available to process this command\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7880855202674866, 0.7880855202674866], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [7, 7], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_LungCancer', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.25, 0.7880855202674866, 0.7880855202674866], 'original_median_shape_after_transp': [266, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2378.0, 'mean': -228.04481506347656, 'median': -91.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 297.0, 'std': 327.0248718261719}}} \n",
      "\n",
      "2024-03-24 09:33:41.824913: unpacking dataset...\n",
      "2024-03-24 09:33:42.307734: unpacking done...\n",
      "2024-03-24 09:33:42.312239: do_dummy_2d_data_aug: False\n",
      "2024-03-24 09:33:42.312239: Using splits from existing split file: D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\\splits_final.json\n",
      "2024-03-24 09:33:42.323430: The split file contains 5 splits.\n",
      "2024-03-24 09:33:42.323430: Desired fold for training: 3\n",
      "2024-03-24 09:33:42.332561: This split has 50 training and 12 validation cases.\n",
      "2024-03-24 09:33:46.873152: Unable to plot network architecture:\n",
      "2024-03-24 09:33:46.873152: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n",
      "2024-03-24 09:33:46.912924: \n",
      "2024-03-24 09:33:46.914487: Epoch 0\n",
      "2024-03-24 09:33:46.919493: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 53, in producer\n",
      "    item = next(data_loader)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\data_loader.py\", line 126, in __next__\n",
      "    return self.generate_train_batch()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\data_loader_2d.py\", line 18, in generate_train_batch\n",
      "    data, seg, properties = self._data.load_case(current_key)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 86, in load_case\n",
      "    data = np.load(entry['data_file'][:-4] + \".npy\", 'r')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py\", line 453, in load\n",
      "    return format.open_memmap(file, mode=mmap_mode,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\format.py\", line 945, in open_memmap\n",
      "    marray = numpy.memmap(filename, dtype=dtype, shape=shape, order=order,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\memmap.py\", line 268, in __new__\n",
      "    mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [WinError 8] Not enough memory resources are available to process this command\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\nnUNetv2_train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 268, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 204, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 1242, in run_training\n",
      "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 196, in __next__\n",
      "    item = self.__get_next_item()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 181, in __get_next_item\n",
      "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
      "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in background worker 0:\n",
      " [WinError 8] Not enough memory resources are available to process this command\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.7880855202674866, 0.7880855202674866], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [7, 7], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_LungCancer', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.25, 0.7880855202674866, 0.7880855202674866], 'original_median_shape_after_transp': [266, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2378.0, 'mean': -228.04481506347656, 'median': -91.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 297.0, 'std': 327.0248718261719}}} \n",
      "\n",
      "2024-03-24 09:33:58.711774: unpacking dataset...\n",
      "2024-03-24 09:33:59.067240: unpacking done...\n",
      "2024-03-24 09:33:59.077243: do_dummy_2d_data_aug: False\n",
      "2024-03-24 09:33:59.083258: Using splits from existing split file: D:\\ScholarX\\nnU-Net\\nnUNet_preprocessed\\Dataset001_LungCancer\\splits_final.json\n",
      "2024-03-24 09:33:59.087817: The split file contains 5 splits.\n",
      "2024-03-24 09:33:59.087817: Desired fold for training: 4\n",
      "2024-03-24 09:33:59.087817: This split has 50 training and 12 validation cases.\n",
      "2024-03-24 09:34:03.756736: Unable to plot network architecture:\n",
      "2024-03-24 09:34:03.763145: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n",
      "2024-03-24 09:34:03.799217: \n",
      "2024-03-24 09:34:03.804235: Epoch 0\n",
      "2024-03-24 09:34:03.808158: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 53, in producer\n",
      "    item = next(data_loader)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\data_loader.py\", line 126, in __next__\n",
      "    return self.generate_train_batch()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\data_loader_2d.py\", line 18, in generate_train_batch\n",
      "    data, seg, properties = self._data.load_case(current_key)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\dataloading\\nnunet_dataset.py\", line 86, in load_case\n",
      "    data = np.load(entry['data_file'][:-4] + \".npy\", 'r')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py\", line 453, in load\n",
      "    return format.open_memmap(file, mode=mmap_mode,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\format.py\", line 945, in open_memmap\n",
      "    marray = numpy.memmap(filename, dtype=dtype, shape=shape, order=order,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\memmap.py\", line 268, in __new__\n",
      "    mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [WinError 8] Not enough memory resources are available to process this command\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\nnUNetv2_train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 268, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\run\\run_training.py\", line 204, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py\", line 1242, in run_training\n",
      "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 196, in __next__\n",
      "    item = self.__get_next_item()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\batchgenerators\\dataloading\\nondet_multi_threaded_augmenter.py\", line 181, in __get_next_item\n",
      "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
      "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n"
     ]
    }
   ],
   "source": [
    "for fold in range(0,5):\n",
    "    command = f\"nnUNetv2_train 001 2d {fold} --npz -device cuda\"\n",
    "    !{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f41e9-cd48-4af0-ac46-4fe14561fde9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
